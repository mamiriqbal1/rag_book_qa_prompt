{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Answer from book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/amir/LLM/llama/lib/python3.11/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain: 0.0.331\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "import langchain\n",
    "\n",
    "# loaders\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# splits\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# prompts\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# vector stores\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# models\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# retrievers\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "print('LangChain:', langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages extracted: 263\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "reader = PdfReader(\"books/Cambridge IGCSE and O Level Computer Science.pdf\")\n",
    "    \n",
    "included_pages_intervals = [[14, 52],\n",
    "                 [57, 82],\n",
    "                 [87, 155],\n",
    "                 [159, 188],\n",
    "                 [192, 225],\n",
    "                 [229, 264],\n",
    "                 [270, 306],\n",
    "                 [311, 348],\n",
    "                 [351, 365],\n",
    "                 [368, 393]]\n",
    "\n",
    "included_pages = []\n",
    "for interval in included_pages_intervals:\n",
    "    l = list(range(interval[0], interval[1]+1))\n",
    "    included_pages = included_pages + l\n",
    "\n",
    "\n",
    "def include_page(page_number):\n",
    "    one_based_page_number = page_number + 1\n",
    "    if one_based_page_number in included_pages:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "parts = []\n",
    "def visitor_body(text, cm, tm, fontDict, fontSize):\n",
    "    if fontDict is not None and '/ILTBBB+OfficinaSansStd' in fontDict['/BaseFont']:\n",
    "        parts.append(text)\n",
    "\n",
    "def extract_single_page(page):\n",
    "    page.extract_text(visitor_text=visitor_body),\n",
    "    text_body = \"\".join(parts)\n",
    "    text_body = text_body.replace('\\n', ' ')\n",
    "    return text_body\n",
    "\n",
    "\n",
    "def extract_pages(pdf_reader, source):\n",
    "    documents = []\n",
    "    \n",
    "    for page_number, page in enumerate(pdf_reader.pages):\n",
    "        if include_page(page_number):\n",
    "            doc = Document(\n",
    "                    page_content = extract_single_page(page),\n",
    "                    metadata={\"source\": source, \"page\": page_number},\n",
    "                    ) \n",
    "            if len(doc.page_content) > 100:\n",
    "                documents.append(doc)\n",
    "            else:\n",
    "                pass\n",
    "                # print('dropped page content: ' + doc.page_content)\n",
    "            global parts\n",
    "            parts =[]\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "documents = extract_pages(reader, \"Cambridge IGCSE and O Level Computer Science.pdf\")\n",
    "\n",
    "print('pages extracted: ' + str(len(documents)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have created 547 chunks from 263 pages\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f'We have created {len(texts)} chunks from {len(documents)} pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "CPU times: user 43.6 s, sys: 3.53 s, total: 47.1 s\n",
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# ### download embeddings model\n",
    "# embeddings = HuggingFaceInstructEmbeddings(\n",
    "#     model_name = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "#     model_kwargs = {\"device\": \"cpu\"}\n",
    "# )\n",
    "\n",
    "# ### create embeddings and DB\n",
    "# vectordb = FAISS.from_documents(\n",
    "#     documents = texts, \n",
    "#     embedding = embeddings\n",
    "# )\n",
    "\n",
    "# ### persist vector database\n",
    "# vectordb.save_local(\"faiss_index_hp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "CPU times: user 310 ms, sys: 117 ms, total: 427 ms\n",
      "Wall time: 461 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### download embeddings model\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "### load vector DB embeddings\n",
    "vectordb : FAISS = FAISS.load_local(\n",
    "    \"faiss_index_hp\",\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='When data is transmitted, there is always a risk that it may be corrupted, lost or  even gained. Errors can occur during data transmission due to:  interference (all types of cable can suffer from electrical interference, which  can cause data to be corrupted or even lost) problems during packet switching (this can lead to data loss – or it is even possible to gain data!)  skewing of data (this occurs during parallel data transmission and can cause data corruption if the bits arrive out of synchronisation). Checking for errors is important since computers are unable to understand text, for example, if the words are not recognised by its built-in dictionary. Look at the following example of some corrupted text:   Whilst you probably had little problem understanding this text, a computer', metadata={'source': 'Cambridge IGCSE and O Level Computer Science.pdf', 'page': 65}),\n",
       " Document(page_content='of the error and are usually automatically generated by the computer. The programmer needs to know how to interpret the hexadecimal error codes. Examples of error codes from a Windows system are shown below:', metadata={'source': 'Cambridge IGCSE and O Level Computer Science.pdf', 'page': 23}),\n",
       " Document(page_content='would be unable to make any sense of it. Data corruption is therefore a very real problem to a computer. Figure 2.13 could be the result of some data corruption following transmission which would make the text unintelligible to a computer. This is why error checking is such an important part of computer technology. The following section considers a number of ways that can be used to check for errors, so that you don’t end up with text as shown in Figure 2.13 above! There are a number of ways data can be checked for errors following  transmission:  parity checks checksum echo check.', metadata={'source': 'Cambridge IGCSE and O Level Computer Science.pdf', 'page': 65}),\n",
       " Document(page_content='other words, a parity check is done in both horizontal and vertical directions). As the following example shows, this method not only identifies that an error has occurred but also indicates where the error is.', metadata={'source': 'Cambridge IGCSE and O Level Computer Science.pdf', 'page': 68})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test if vector DB was loaded correctly\n",
    "results = vectordb.similarity_search('error detection')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Don't try to make up an answer, if you don't know just say that you don't know.\n",
    "Answer in the same language the question was asked.\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.vectorstore import VectorStoreRetriever\n",
    "\n",
    "\n",
    "retriever : VectorStoreRetriever = vectordb.as_retriever(search_kwargs = {\"k\": 3, \"search_type\" : \"similarity\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Don't try to make up an answer, if you don't know just say that you don't know.\n",
      "Answer in the same language the question was asked.\n",
      "Use only the following pieces of context to answer the question at the end.\n",
      "\n",
      " Parity checking is one method used to check whether data has been changed or  corrupted following data transmission. This method is based on the number of 1-bits in a byte of data. The parity can be either called EVEN  (that is, an even number of 1-bits in the  byte) or ODD  (that is, an odd number of 1-bits in the byte). One of the bits in  the byte (usually the most significant bit or left-most bit) is reserved for a parity  bit. The parity bit is set according to whether the parity being used is even or  odd. For example, consider the byte: In this example, if the byte is using even parity, then the parity bit needs to be set to 0, since there is already an even number of 1-bits in the byte (four 1-bits). We thus get: In this example, if the byte is using odd parity, then the parity bit needs to be set to 1, since we need to have an odd number of 1-bits in the byte. We thus get: Before data is transferred, an agreement is made between sender and receiver regarding which type of parity is being used. Parity checks are therefore being used as a type of transmission protocol. other words, a parity check is done in both horizontal and vertical directions). As the following example shows, this method not only identifies that an error has occurred but also indicates where the error is.\n",
      "\n",
      "Question: What is parity check\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "query = 'What is parity check'\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "merged_context = ''\n",
    "for doc in docs:\n",
    "    merged_context = merged_context + ' ' + doc.page_content\n",
    "\n",
    "final_prompt = prompt_template.format(context=merged_context, question=query)\n",
    "print(final_prompt)\n",
    "\n",
    "import pyperclip\n",
    "pyperclip.copy(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
